# Лабораторная работа(3): Анализ и сравнение функций активации в нейронных сетях для задач классификации

Этот репозиторий содержит пример использования различных функций активации в нейронных сетях для классификации рукописных цифр с использованием датасета MNIST.

## Обзор

Цель этого проекта - продемонстрировать влияние различных функций активации на процесс обучения и производительность нейронных сетей. Для обучения и оценки используется датасет MNIST, содержащий изображения цифр от 0 до 9.

## Модели

Реализованы несколько моделей, каждая из которых использует различную функцию активации:

- Сигмоида
- Гиперболический тангенс (Tanh)
- Rectified Linear Unit (ReLU)
- Leaky Rectified Linear Unit (Leaky ReLU)
- Parametric Rectified Linear Unit (PReLU)
- Gaussian Error Linear Unit (GELU)

Каждая модель компилируется с оптимизатором Adam, функцией потерь categorical crossentropy и метрикой accuracy.

## Обучение

Модели обучаются на тренировочном датасете MNIST с размером пакета 64 в течение 10 эпох. Процесс обучения визуализируется графиками потерь и точности для тренировочного и валидационного наборов данных.

## Структура файлов

- `main.py`: Основной скрипт с определением моделей, обучением и оценкой.
- `utils.py`: Вспомогательные функции для построения графиков обучения.

## Использование

Для запуска кода убедитесь, что у вас установлены необходимые зависимости:

```bash
pip install matplotlib tensorflow
